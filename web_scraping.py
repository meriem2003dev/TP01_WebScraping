# -*- coding: utf-8 -*-
"""web scraping.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NAplWbmoGUyuJiqXFHu2PUwoNrmeenfK
"""

import requests
from bs4 import BeautifulSoup
import csv
from urllib.parse import urljoin
from google.colab import files

base_url = "https://books.toscrape.com/"

all_data = []
report_data = []

# 1) Ø§Ø³ØªØ®Ø±Ø§Ø¬ ÙƒÙ„ Ø§Ù„Ø£Ù‚Ø³Ø§Ù… Ù…Ù† Ø§Ù„ØµÙØ­Ø© Ø§Ù„Ø±Ø¦ÙŠØ³ÙŠØ©
response = requests.get(base_url)
soup = BeautifulSoup(response.text, "html.parser")

categories = soup.find("ul", class_="nav nav-list").find_all("a")
categories = categories[1:]  # Ù†ØªØ¬Ø§Ù‡Ù„ "Books" Ø§Ù„Ø¹Ø§Ù…Ø©

for cat in categories:
    category_name = cat.text.strip()
    category_url = urljoin(base_url, cat["href"])
    page_url = category_url

    books_count = 0
    pages_count = 0

    while True:
        response = requests.get(page_url)
        soup = BeautifulSoup(response.text, "html.parser")

        books = soup.find_all("article", class_="product_pod")
        pages_count += 1

        for book in books:
            title = book.h3.a["title"].strip()
            price = book.find("p", class_="price_color").text.strip()
            availability = book.find("p", class_="instock availability").text.strip()
            star_rating = book.p["class"][1]
            link = urljoin(page_url, book.h3.a["href"])

            all_data.append([category_name, title, price, availability, star_rating, link])
            books_count += 1

        # ØªØ­Ù‚Ù‚ Ù…Ù† ÙˆØ¬ÙˆØ¯ ØµÙØ­Ø© ØªØ§Ù„ÙŠØ©
        next_btn = soup.find("li", class_="next")
        if next_btn:
            page_url = urljoin(page_url, next_btn.a["href"])
        else:
            break

    # Ø£Ø¶Ù ØªÙ‚Ø±ÙŠØ± Ø§Ù„Ù‚Ø³Ù…
    report_data.append([category_name, books_count, pages_count])

# 2) Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ÙƒØªØ¨
books_file = "books_all_categories.csv"
with open(books_file, "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["Category", "Title", "Price", "Availability", "Star Rating", "Link"])
    writer.writerows(all_data)

# 3) Ø­ÙØ¸ Ù…Ù„Ù Ø§Ù„ØªÙ‚Ø±ÙŠØ±
report_file = "report.csv"
with open(report_file, "w", newline="", encoding="utf-8-sig") as f:
    writer = csv.writer(f)
    writer.writerow(["Category", "Books Count", "Pages Count"])
    writer.writerows(report_data)

print(f"âœ… ØªÙ… Ø§Ø³ØªØ®Ø±Ø§Ø¬ {len(all_data)} ÙƒØªØ§Ø¨ Ù…Ù† {len(categories)} Ù‚Ø³Ù…")
print(f"ğŸ“Š ØªÙ… Ø­ÙØ¸ ØªÙ‚Ø±ÙŠØ± Ø¨Ø¹Ø¯Ø¯ Ø§Ù„ÙƒØªØ¨ ÙˆØ§Ù„ØµÙØ­Ø§Øª ÙÙŠ {report_file}")

# 4) ØªÙ†Ø²ÙŠÙ„ Ø§Ù„Ù…Ù„ÙØ§Øª
files.download(books_file)
files.download(report_file)